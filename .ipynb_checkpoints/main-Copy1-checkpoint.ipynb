{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import librosa\n",
    "import pysrt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "random_state = 123       \n",
    "tdsv = False         # Text dependent or not\n",
    "sr = 16000           # Sample Rate\n",
    "nfft = 512           # FFT Size\n",
    "window = 0.0025      # Window Lenght 25ms\n",
    "hop = 0.01           # Hop Size 10ms\n",
    "tdsv_frame = 160     # Frame number of utterance of tdsv\n",
    "tisv_frame = 160     # Max frame number of utterances of tdsv\n",
    "tisv_frame_min = 50  # min frame length to confirm with 'd-vector v-3 model'\n",
    "\n",
    "# Model Parameters\n",
    "hidden = 768        # Hidden State dimension of LSTM\n",
    "proj = 256          # Hidden State dimension of LSTM\n",
    "num_layer = 3       # Number of LSTM layers\n",
    "restore = False     # Restore model or not\n",
    "model_num = 3       # Number of ckpt file to load \n",
    "\n",
    "# Main Parameters\n",
    "number_of_speakers = 2   # Number of speakers\n",
    "srt_path = 'output-files/str_file.en.srt' # Enter the absolute path to srt file\n",
    "log_path = 'output-files/main.logs' # \n",
    "output_dir = 'output-files/output.json' # Path to save the output json file\n",
    "#audio_file = '/home/bruno/datasets/YouTube/Obama/BarackObama.wav' # Enter the absolute path to audio file\"\n",
    "audio_file = '/home/bruno/datasets/mixed-musan/wav16/two_speakers_0.wav'\n",
    "model_path = 'models/model.ckpt-46'\n",
    "\n",
    "vad_threshold = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\" normalize the last dimension vector of the input matrix\n",
    "    :return: normalized input\n",
    "    \"\"\"\n",
    "    return x/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keep_dims=True)+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "# embedding lstm (3-layer default)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=hidden, num_proj=proj) for i in range(num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_tensorflow = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, model_path)\n",
    "    logging.info(\"loading audio\")\n",
    "    audio_path = audio_file\n",
    "    utter, sr = librosa.core.load(audio_path, sr=sr)        # load audio\n",
    "    utter_min_len = (tisv_frame_min * hop + window) * sr    # lower bound of utterance length\n",
    "    # Get the duration\n",
    "    duration = librosa.get_duration(utter, sr)\n",
    "    # Duration of each window\n",
    "    duration_per_frame = (duration / utter.shape[0])\n",
    "    logging.info(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "    tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "    intervals = librosa.effects.split(utter, top_db=vad_threshold)         # voice activity detection\n",
    "\n",
    "    all_data = []\n",
    "    logging.info('Converting intervals to embeddings')\n",
    "    selected_intervals_idx = []\n",
    "    for idx, current_interval in enumerate(intervals):\n",
    "        if (current_interval[1]-current_interval[0]) > utter_min_len:\n",
    "            # Save these selected intervals, as shorter ones are ignored\n",
    "            selected_intervals_idx.append(idx)\n",
    "            utterances_spec = []\n",
    "            utter_part = utter[current_interval[0]:current_interval[1]]         # save first and last 160 frames of spectrogram.\n",
    "            S = librosa.core.stft(y=utter_part, n_fft=nfft,\n",
    "                                    win_length=int(window * sr), hop_length=int(hop * sr))\n",
    "            S = np.abs(S) ** 2\n",
    "            mel_basis = librosa.filters.mel(sr=sr, n_fft=nfft, n_mels=40)\n",
    "            S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "            utterances_spec.append(S[:, :tisv_frame])\n",
    "            utterances_spec.append(S[:, -tisv_frame:])\n",
    "            utterances_spec = np.array(utterances_spec)\n",
    "            utter_batch = np.transpose(utterances_spec, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "\n",
    "            data = sess.run(embedded, feed_dict={verif:utter_batch})\n",
    "            all_data.extend(data)\n",
    "data = np.array(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering\n",
    "\n",
    "## Cossine Similarity\n",
    "\n",
    "Matriz de afinidade A, $A_{ij}$ é a the cossine similarity entre o $i^{th}$ e o $j^{th}$ parâmetro segmentado (embedding) onde $i \\neq j$ <br> <br>\n",
    "$A_{ii} = max_{j \\neq i} A_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = np.dot(data, data.T)  # Cossine similarity\n",
    "square_mag = np.diag(similarity)   # Squared magnitude of preference vectors (number of occurrences) (diagonals are ai*ai)\n",
    "inv_square_mag = 1 / square_mag    # Inverse squared magnitude\n",
    "inv_square_mag[np.isinf(inv_square_mag)] = 0 # if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
    "inv_mag = np.sqrt(inv_square_mag)  # Inverse of the magnitude\n",
    "cosine = similarity * inv_mag      # Cosine similarity (elementwise multiply by inverse magnitudes)\n",
    "A =  cosine.T * inv_mag            # Final step cossine sim\n",
    "np.fill_diagonal(A, -1000)         # Fill the diagonals with very large negative value\n",
    "np.fill_diagonal(A, A.max(axis=1)) # Fill the diagonals with the max of each row\n",
    "A = (1-A)/2                        # Final step in cossine sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de afinidade\n",
    "plt.imshow(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian blur\n",
    "sigma = 0.5  # we will select sigma as 0.5\n",
    "A_gau = gaussian_filter(A, sigma)\n",
    "\n",
    "# para vizialização\n",
    "A_gau_sigma1 = gaussian_filter(A, 1)\n",
    "A_gau_sigma2 = gaussian_filter(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig, ax = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "\n",
    "ax[0].imshow(A, cmap='gray')\n",
    "ax[0].set_title('Affinity Matrix')\n",
    "\n",
    "ax[1].imshow(A_gau, cmap='gray')\n",
    "ax[1].set_title('Gaussian blur sigma = 0.5')\n",
    "\n",
    "ax[2].imshow(A_gau_sigma1, cmap='gray')\n",
    "ax[2].set_title('Gaussian blur sigma = 1')\n",
    "\n",
    "ax[3].imshow(A_gau_sigma2, cmap='gray')\n",
    "ax[3].set_title('Gaussian blur sigma = 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrization: $Y_{ij} = max(X_{ij}, X_{ji})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_multiplier = 0.01  # Thresholding using multiplier = 0.01\n",
    "A_thresh = A_gau * threshold_multiplier# Symmetrization\n",
    "A_sym = np.maximum(A_thresh, A_thresh.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "\n",
    "ax[0].imshow(A, cmap='gray')\n",
    "ax[0].set_title('Affinity Matrix')\n",
    "\n",
    "ax[1].imshow(A_gau, cmap='gray')\n",
    "ax[1].set_title(f'Gaussian blur = {sigma}')\n",
    "\n",
    "ax[2].imshow(A_thresh, cmap='gray')\n",
    "ax[2].set_title(f'Threshold = {threshold_multiplier}')\n",
    "\n",
    "ax[3].imshow(A_sym, cmap='gray')\n",
    "ax[3].set_title(f'Symmetrization')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion: \n",
    " $Y = XX^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion\n",
    "A_diffusion = A_sym * A_sym.T # Row-wise matrix Normalization\n",
    "Row_max = A_diffusion.max(axis=1).reshape(1, A_diffusion.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row-wise Max Normalization: \n",
    "$Y_{ij} = X_{ij}/max_kX_{ik}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_norm = A_diffusion / Row_max.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(16, 4))\n",
    "\n",
    "ax[0].imshow(A, cmap='gray')\n",
    "ax[0].set_title('Affinity Matrix')\n",
    "\n",
    "ax[1].imshow(A_gau, cmap='gray')\n",
    "ax[1].set_title(f'Gaussian blur = {sigma}')\n",
    "\n",
    "ax[2].imshow(A_thresh, cmap='gray')\n",
    "ax[2].set_title(f'Threshold = {threshold_multiplier}')\n",
    "\n",
    "ax[3].imshow(A_sym, cmap='gray')\n",
    "ax[3].set_title(f'Symmetrization')\n",
    "\n",
    "ax[4].imshow(A_diffusion, cmap='gray')\n",
    "ax[4].set_title(f'Diffusion')\n",
    "\n",
    "ax[5].imshow(A_norm, cmap='gray')\n",
    "ax[5].set_title(f'Normalized')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Decomposition\n",
    "After  all  refinement  operations  have  been  applied,  perform eigen-decomposition on the refined affinity matrix. Let the $n$\n",
    "eigen-values be: $λ1> λ2> ··· > λn$. We use the maximal\n",
    "eigen-gap to determine the number of clusters $k$ <br>\n",
    "$k = arg max_{\\substack{1 \\leq k \\leq n}} \\frac{\\lambda_{k}}{\\lambda_{k + 1}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen decomposition\n",
    "eigval, eigvec = np.linalg.eig(A_norm)\n",
    "# Since eigen values cannot be negative for Positive semi definite matrix, the numpy returns negative values, converting it to positive\n",
    "eigval = np.abs(eigval)\n",
    "sorted_eigval_idx = np.argsort(eigval)[::-1]\n",
    "sorted_eigval = np.sort(eigval)[::-1]\n",
    "# For division according to the equation\n",
    "eigval_shifted = np.roll(sorted_eigval, -1)\n",
    "# Thresholding eigen values because we don't need very low eigan values due to errors\n",
    "eigval_thresh = 0.1\n",
    "sorted_eigval = sorted_eigval[sorted_eigval > eigval_thresh]\n",
    "eigval_shifted = eigval_shifted[:sorted_eigval.shape[0]]\n",
    "# Don't take the first value for calculations, if first value is large, following equation will return k=1, and we want more than one clusters\n",
    "# Get the argmax of the division, since its 0 indexed, add 1\n",
    "k = np.argmax(sorted_eigval[1:]/eigval_shifted[1:]) + 2\n",
    "logging.debug(f'Number of Eigen vectors to pick: {k}')\n",
    "# Get the indexes of eigen vectors\n",
    "idexes = sorted_eigval_idx[:k]\n",
    "A_eigvec = eigvec[:, idexes]\n",
    "A_eigvec = A_eigvec.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of cluster: {k}')\n",
    "print(A_eigvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "ax[0].scatter(A_eigvec[:,0], A_eigvec[:,1])\n",
    "ax[1].scatter(eigvec[:,1], eigvec[:,2])\n",
    "ax[2].scatter(eigvec[:,2], eigvec[:,3])\n",
    "ax[2].scatter(eigvec[:,3], eigvec[:,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_eigvec_norm = sk_normalize(A_eigvec) # l2 normalized\n",
    "kmeans = KMeans(n_clusters=number_of_speakers, init='k-means++', random_state=random_state)\n",
    "kmeans.fit(A_eigvec)\n",
    "labels = kmeans.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_srt_json = os.path.join(output_dir, os.path.basename(srt_path) + '.json')\n",
    "output_wav_json = os.path.join(output_dir, os.path.basename(srt_path) + '.wav.json')\n",
    "OL_INDICATOR = 'OL'\n",
    "SIL_INDICATOR = -1\n",
    "json_data = []\n",
    "for idx, i in enumerate(selected_intervals_idx):\n",
    "    start = str(datetime.timedelta(seconds = intervals[i][0] * duration_per_frame))\n",
    "    end = str(datetime.timedelta(seconds = intervals[i][1] * duration_per_frame))\n",
    "    speaker = labels[idx*2]\n",
    "    if labels[idx*2] != labels[(idx*2)+1]:\n",
    "        speaker = 'OL' # possible overlap\n",
    "    json_data.append({\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'speaker': str(speaker)\n",
    "    })\n",
    "    \n",
    "# Save the output to json\n",
    "with open(output_wav_json, 'w') as f:\n",
    "    json.dump(json_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_json = {}\n",
    "json_data = []\n",
    "subs = pysrt.open(output_wav_json, encoding=\"utf-8\")\n",
    "convert_to_ms = lambda st: (st.hours * 60 * 60 * 1000) + \\\n",
    "                            (st.minutes * 60 * 1000) +\\\n",
    "                            (st.seconds * 1000) +\\\n",
    "                            (st.milliseconds)\n",
    "for sub in subs:\n",
    "    start_in_ms = convert_to_ms(sub.start)\n",
    "    end_in_ms = convert_to_ms(sub.end)\n",
    "    speakers = []\n",
    "    for idx, i in enumerate(selected_intervals_idx):\n",
    "        start = intervals[i][0] * duration_per_frame * 1000\n",
    "        end = intervals[i][1] * duration_per_frame * 1000\n",
    "        if start_in_ms <= start <= end_in_ms:\n",
    "            speaker = int(labels[idx*2])\n",
    "            if labels[idx*2] != labels[(idx*2)+1]:\n",
    "                speaker = OL_INDICATOR # possible overlap\n",
    "            speakers.append(speaker)\n",
    "    json_data.append({\n",
    "        \"index\": sub.index,\n",
    "        \"start\": sub.start.to_time().strftime(\"%H:%M:%S,%f\")[:-3],\n",
    "        \"end\": sub.end.to_time().strftime(\"%H:%M:%S,%f\")[:-3],\n",
    "        'speakers': np.unique(speakers).tolist(),\n",
    "        'speakers_distribution': speakers,\n",
    "        'text': sub.text\n",
    "    })\n",
    "metadata = {\n",
    "    \"overlap_indicator\": OL_INDICATOR,\n",
    "    \"duration\": duration,\n",
    "    \"class_names\": np.unique(labels).tolist(),\n",
    "    \"num_of_speakers\": len(set(labels)),\n",
    "    \"silence_indicator\": SIL_INDICATOR\n",
    "}\n",
    "complete_json[\"metadata\"] = metadata\n",
    "complete_json[\"srt\"] = json_data\n",
    "# Save the output to json\n",
    "with open(output_srt_json, 'w') as f:\n",
    "    json.dump(complete_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
